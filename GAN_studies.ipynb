{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of GAN-studies.ipynb","provenance":[{"file_id":"1Hz5b_-M7oXUtQJqWQDZXOq-R9FMnYbiP","timestamp":1575509995199}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zZFvombhn7bo","colab_type":"text"},"source":["# A Fast Introduction to GAN by Ege Beyazit\n","In this notebook, we study GAN: Generative Adversarial Networks (Goodfelllow, 2014). We try to highlight the most important properties in order to jump into GAN research ASAP.\n","\n","The list of references we use for this notebook is:\n","1. NIPS'16 Tutorial on how GANs work: https://arxiv.org/abs/1701.00160\n","2. An Annotated Proof of GANs: https://srome.github.io/An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes/\n","3. NIPS'16 Tutorial on how to train GANs: https://github.com/soumith/ganhacks\n","4. Learning Class-Conditional GANs with Active Sampling, KDD'19: https://www.kdd.org/kdd2019/accepted-papers/view/learning-class-conditional-gans-with-active-sampling\n","5. Improved Techniques for Training GANs (1000+ citations): https://arxiv.org/pdf/1606.03498.pdf\n","## 0. Why use GANs?\n","GANs were designed to avoid some common disadvantages of other generative models. Specifically:\n","1. They can generate samples in parallel, unlike the models like FVBNs that require sequential sampling to converge.\n","2. The generator function has very few restrictions unlike RBMs (can only use the distributions that admits Markov Chain sampling) and non-linear ICAs (generator must be invertible and latent code must have the same dimensionality as the input).\n","3. No Markov Chains are needed, unlike RBMs and GSNs.\n","4. No variational bound is needed. Some model families can be used with GANs are known to be asymptotically consistent -> universal approximators.\n","5. GANs usually produce better samples than other generators.\n","\n","## 1. How do GANs work?\n","1. GAN framework is a game between two players: \n","  - *generator*: tries to create samples that look like the training data, and \n","  - *discriminator*: tries to determine whether the samples are real or fake.\n","\n","2. GANs are structured (z -> x, directed where every latent variable influences every observed variable) probabilistic models containing latent variables $\\mathbf{z}$ and observed variables $\\mathbf{x}$.\n","3. The two players are represented by two functions. Both of them are differentiable w.r.t. both the inputs and their parameters:\n","  - Generator: $G(\\mathbf{z}; \\mathbf{\\theta}^{(G)})$, where $\\mathbf{z}$ is input noise and $\\mathbf{\\theta}^{(G)}$ are the model parameters,\n","  - Discriminator: $D(\\mathbf{x}; \\mathbf{\\theta}^{(D)})$, where $\\mathbf{x}$ are the observed variables and $\\mathbf{\\theta}^{(D)}$ are the model parameters. \n","4. Both players try to minimize their own cost functions:\n","   - Generator: $J^{(G)}(\\mathbf{\\theta}^{(D)}, \\mathbf{\\theta}^{(G)})$ only having control on $G$ parameters for minimization, \n","   - Discriminator: $J^{(D)}(\\mathbf{\\theta}^{(D)}, \\mathbf{\\theta}^{(G)})$ only having control on $D$ parameters for minimization.\n","   - **Therefore this is a game, more than an optimization problem.**\n","4. The solution to an optimization problem is a local minima, the solution to a game is *Nash equilibria*. **Specifically for GANs, the Nash equilibrium is a tuple $(\\mathbf{\\theta}^{(D)}, \\mathbf{\\theta}^{(G)})$ that is a local minimum of $J^{(D)}$ with respect to $\\mathbf{\\theta}^{(D)}$ and a local minimum of $J^{(G)}$ with respect to $\\mathbf{\\theta}^{(G)}$.**\n","\n","### 1.1. The Generator\n","1. When $\\mathbf{z}$ is sampled from some prior distribution, $G(\\mathbf{z})$ yields a sample of $\\mathbf{x}$ drawn from $p_{model}$. **$G$ is typically a deep neural network.**\n","2. Inputs to the function $G$ can be to any layer of the network that represents it. For example,\n","  - We can partition $\\mathbf{z}$ into two vectors $\\mathbf{z}^{(1)}$ and  $\\mathbf{z}^{(2)}$, then feed $\\mathbf{z}^{(1)}$ to the input and $\\mathbf{z}^{(2)}$ to the output layer. If $\\mathbf{z}^{(2)}$ is Gaussian, this makes $\\mathbf{x}$ conditionally Gaussian given $\\mathbf{z}^{(1)}$.\n","3. If we want $p_{model}$ to have full support on $\\mathbf{x}$ space, we need the dimension of $\\mathbf{z}$ to be at least as large as the dimension of $\\mathbf{x}$, and $G$ must be differentiable.\n","\n","### 1.2. The Training Process\n","Simultaneous SGD, for each step:\n","  1. Sample a minibatch of $\\mathbf{x}$ values from the dataset and,\n","  2. Sample a minibatch of $\\mathbf{z}$ values drawn from the model's prior over latent variables.\n","  3. Do both gradient steps simultaneously: update $\\mathbf{\\theta}^{(D)}$ to reduce $J^{(D)}$, and update $\\mathbf{\\theta}^{(G)}$ to reduce $J^{(G)}$.\n","\n","## 2. Cost Functions\n","### 2.1. Discriminator Cost $J^{(D)}$\n","1.  All games designed for GANs use the same cost for discriminator.\n","2.  The cost for the discriminator is:\n","\\begin{equation*}\n","J^{(D)}(\\mathbf{\\theta}^{(D)}, \\mathbf{\\theta}^{(G)}) = \n","-\\frac{1}{2}\\mathbb{E}_{\\mathbf{x} \\sim p_{data}}\\log D(\\mathbf{x}) \n","-\\frac{1}{2}\\mathbb{E}_{\\mathbf{z}}\\log(1-D(G(\\mathbf{z}))).\n","\\end{equation*}\n","Note that this is a straightforward negative log likelihood function that receives data from two different sources. The objective is maximizing the likelihood when data from original distribution is received, and minimizing it when data from the generator is received. So, the classifier is trained on two minibatches of data.\n","\n","### 2.2 Generator Cost $J^{(G)}$\n","There are many ways to define a cost function for the generator, based on the specific task at hand. \n","The simplest version of the generator cost is the opposite of the discriminator's:\n","\\begin{equation*}\n","J^{(G)} = - J^{(D)}.\n","\\end{equation*}\n","This is also knows as a *zero-sum* game, in which the sum of all players' costs is always zero.\n"," \n"," ## 3. Game Design\n"," ### 3.1. Minimax\n","Using the *zero-sum* design, the optimization problem becomes a minimax problem because the solution involves minimization in an outer loop and maximization in an inner loop:\n","\\begin{equation*}\n","\\mathbf{\\theta}^{(G)*} = \\text{argmin}_{\\mathbf{\\theta}^{(G)}}\\text{max}_{\\mathbf{\\theta}^{(D)}} - J^{(D)}(\\mathbf{\\theta}^{(D)}, \\mathbf{\\theta}^{(G)})\n","\\end{equation*}\n","- This version of the game has nice propertiesfor theoretical analysis but does not perform well in practice.\n","  - Learning resembles minimizing the Jensen-Shannon divergence between the data and the model distribution.\n","  - Game converges to its equilibrium if both players' policies can be updated directly in function space.\n","  - In practice, players are deep neural nets and updates are made in parameter space. So, the results do not apply.\n","\n","### 3.2 Heuristic Non-saturating Game\n","In the minimiax game, discriminator minimizes a cross-entropy and the generator maximizes the same-cross entropy. This setting is problematic because when the discriminator acquires low loss (near zero), the generator's gradients will vanish (simply flipping the sign of a near zero value won't give us a good error).\n","\n","Instead of flipping the sign on the discriminator's cost to obtain a cost for the generator, we flip the target used to construct the cross-entropy cost. Then, the generator cost becomes:\n","\\begin{equation*}\n","J^{(G)} = -\\frac{1}{2}\\mathbb{E}_{\\mathbf{z}}\\log D(G(z)).\n","\\end{equation*}\n","**What is the difference?**\n","- In the minimax game, the generator minimizes the log-probability of the discriminator being correct.\n","- In this game, the generator maximizes the log probability of the discriminator being mistaken.\n","- This version of the game is a heuristic modification to the original, therefore it is no longer zero-sum. **However, it ensures that each player has a strong gradient when that player is losing the game.**\n","\n","### 3.3. Maximum Likelihood Game\n","It is also possible to maximize likelihood, or minimize the KL-Divergence between the data and the model. There are variety of methods to do that, however we omit this for now because heuristic non-saturating game has shown to perform better. For details of the maximum likelihood game and why it fails, check page 26, figure 16 of the NIPS tutorial on GANs by Ian Goodfellow.\n","\n","## 4. Tips and Tricks\n","It is hard to train GANs. We explore some tips and tricks that can possibly help us. For more, check this NIPS workshop on how to train GANS: \n","https://github.com/soumith/ganhacks.\n","### 4.1. Train with Labels\n","1. Using labels always results in a dramatic improvement (https://www.kdd.org/kdd2019/accepted-papers/view/learning-class-conditional-gans-with-active-sampling).\n","2. Training **only** the discriminator on labeled data improves the quality (https://arxiv.org/pdf/1606.03498.pdf).\n","\n","### 4.2. One-sided Label Smoothing\n","DNNs are prone to producing highly confident outputs that identify the correct class but with too extreme of a probability. One-sided label smoothing punishes extremely high confidence predictions if the sample was not fake. This acts as a **regularizer** because it does not encourage the model to choose an incorrect class on the training set, but only to reduce the confidence in the correct class (standard regularizers punish confidence that may encourage incorrect classifications).\n","\n","### 4.3. Virtual Batch Normalization\n","Batch normalization is used to improve optimization of the model, by reparameterizing the model so that the mean and variance of each feature are determined by a single mean parameter and a single variance parameter associated with that feature. This simplifies the interactions between weights, because otherwise a complex interaction between all of the weights of all of the layers would determine the mean and variances.\n","\n","### 4.4 Balancing $G$ and $D$\n","- Some researchers think it is important to balance the training of the generator and the discriminator powers. Sometimes when discriminator starts to acqiure very low loss, the gradient for the generator can vanish because the discriminator is too accurrate. **The right way to solve this problem is not to limit the power of the discriminator, but to use a parameterization of the game where the gradient does not vanish.**\n","\n","- To balance the opposite case, where the gradient for the generator becomes very large because the discriminator is too confident, **use label smoothing.**\n","\n","- Another way to balance the generator and discriminator, we can choose different model sizes. In practice, the discriminator is usually deeper and sometimes has more filters per layer than the generator. **Reasons can be:**\n","  - It is important for discriminator to be correct.\n","  - Artifact of the mode collapse problem: generators tend not to use their full capcity with current training methods. So, there is no point to increase their capacity.\n","\n","## 5. Research Frontiers\n","### 5.1. Non-convergence\n","- Optimization algorithms usually make downhill progress in the loss surface. On the other hand because GANs play a minimax type of game, each update can direct the optimization towards or away from the equillibrium.\n","- There is no theoretical proof that GANs should converge, using the gradient based update rules. In practice, GANs often seem to oscillate: they progress from generating one kind of sample from another kind without reaching an equilibrium.\n","\n","#### 5.1.1. Mode Collapse\n","- The most common form of harmful non-convergence. This  is a problem that occurs when the generator learns to map several different input $z$ values to the same output point.\n","\n","- In practice, complete mode collapse is rare, but partial mode collapse is commmon. E.g., generator makes multiple images that contain the same color or texture themes, multiple images containing different views of the same dog.\n","\n","- Mode collapse may arise because simultaneous gradient descent does not clearly privilege the minmax over maxmin. It ofthen behaves like it is solving the maxmin problem.\n","\n","- Potential solutions attemps that work fairly well for mode collapse are:\n","minibatch features (Salimans et al., 2016), unrolled GANs (Metz et al., 2016).\n","\n","#### 5.1.2 Other Stuff\n","Some other important things to mention are:\n","1. There is no clear evaluation metric for generated samples for GANs. Models that obtain good likelihood can generate bad samples, models that generate good samples can have poor likelihood. GANs are especially harder because it can be difficult to estimate the likelihood of GANs.\n","\n","2. Discrete outputs. Generators must be differentiable. Therefore, generator can not produce discrete data such as one-hot word or character representations. Some possible ways to address this are:\n","  - Using the REINFORCE algorithm,\n","  - Using the concrete distribution or Gumbel-softmax,\n","  - Training the generate to sample continuous values that can be decoded to discrete ones (sampling word embeddings directly).\n","\n","3. State-of-the-art **semi-supervised** learner is a GAN (Salimans et al., 2016).\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"aWL3EeQAjqf4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}