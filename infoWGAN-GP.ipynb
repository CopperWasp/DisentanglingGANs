{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"infoWGAN-GP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO8dDNFBUMUPSyeKQ3SMfAT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JY5o-6FFuH5-","colab_type":"code","cellView":"form","outputId":"945bc25b-d738-4db2-86d2-f1b610e82c28","executionInfo":{"status":"ok","timestamp":1580595520492,"user_tz":360,"elapsed":4198,"user":{"displayName":"Ege BeyazÄ±t","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDqLV_4GuV8zQApBx0FQZnBwgtdNoIo9u8C9JCluw=s64","userId":"02753041663080474765"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#@title Imports\n","%tensorflow_version 1.x \n","import numpy as np\n","import scipy.misc\n","import imageio\n","import argparse\n","from PIL import Image \n","import math\n","import tensorflow as tf\n","from keras.datasets import mnist\n","from tensorflow.contrib.layers import conv2d, conv2d_transpose, layer_norm, fully_connected\n","import random\n","import os\n","import time"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8wsZjHNsuC4c","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Parameters\n","BATCH_SIZE = 128\n","IMG_DIM    = (28, 28, 1)\n","Z_DIM      = 80\n","ZC_DIM     = 10\n","ZU_DIM     = 2\n","\n","OUTPUT_DIM = int(np.prod(IMG_DIM))\n","LAMBDA     = 10\n","ITERS      = 1  # 20001\n","CRITIC_ITER= 5\n","\n","leakyrelu_alpha    = 0.1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qL2WyXKtiNs","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Network functions\n","def save_images(X, save_path):\n","    # [-1, 1] -> [0,255]\n","    X = (np.abs(127.5 * X +  127.4999)).astype('uint8')\n","    \n","    n_samples = X.shape[0]\n","    rows = int(np.sqrt(n_samples))\n","    while n_samples % rows != 0:\n","        rows -= 1\n","\n","    nh, nw = rows, n_samples//rows\n","    \n","    if X.ndim == 2:\n","        X = np.reshape(X, (X.shape[0], int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1]))))\n","\n","    if X.ndim == 4:\n","        h, w = X[0].shape[:2]\n","        img = np.zeros((h*nh, w*nw, 3))\n","        \n","    elif X.ndim == 3:\n","        h, w = X[0].shape[:2]\n","        img = np.zeros((h*nh, w*nw))\n","\n","    for n, x in enumerate(X):\n","        j = n//nw\n","        i = n%nw\n","        img[j*h:j*h+h, i*w:i*w+w] = x\n","\n","    imageio.imwrite(save_path, img.astype('uint8'))\n","\n","\n","def lrelu(x):\n","    return tf.nn.relu(x) - leakyrelu_alpha * tf.nn.relu(-x)\n","\"\"\" Model Definitions \"\"\"\n","\n","\n","def generator_tf(x, reuse = True):\n","    with tf.variable_scope(\"Generator\", reuse = reuse):\n","        x = tf.identity(x, name=\"input\")\n","        x = tf.layers.dense(x, 1024, activation=tf.nn.relu)\n","        x = tf.layers.dense(x, 7 * 7 * 128 , activation=tf.nn.relu)\n","        x = tf.reshape(x, [-1, 7, 7, 128])\n","        x = tf.layers.conv2d_transpose(x, 64, 4, 2, padding='same', activation=tf.nn.relu)\n","        x = tf.layers.conv2d_transpose(x, 1, 4, 2,  padding='same', activation=tf.nn.tanh)\n","        x = tf.identity(x, name=\"output\")\n","                \n","        return x\n","\n","\n","def d_tf(x, reuse = True):\n","    with tf.variable_scope(\"Discriminator\", reuse = reuse):\n","        x = tf.identity(x, name=\"input\")\n","        x = tf.layers.conv2d(x,     64,  4, 2, padding='same', activation=lrelu)\n","        x = tf.layers.conv2d(x,     128, 4, 2, padding='same', activation=lrelu)\n","        x = tf.contrib.layers.flatten(x)\n","        x = tf.layers.dense(x, 1024, activation=lrelu)\n","        x = tf.layers.dense(x, 1)\n","        x = tf.identity(x, name=\"output\")\n","        return x\n","\n","\n","def q_tf(x, reuse = True):\n","    with tf.variable_scope(\"Q\", reuse = reuse):\n","        x = tf.identity(x, name=\"input\")\n","        x = tf.layers.conv2d(x,     64,  4, 2, padding='same', activation=lrelu)\n","        x = tf.layers.conv2d(x,     128, 4, 2, padding='same', activation=lrelu)\n","        x = tf.contrib.layers.flatten(x)\n","        x = tf.layers.dense(x, 1024, activation=lrelu)        \n","        x = fully_connected(x, ZC_DIM + ZU_DIM , activation_fn=None)\n","        x = tf.identity(x, name=\"output\")\n","        return x    \n","\n","\n","def q_cost_tf(z, q):\n","    # categorical part\n","    z_cat = z[:, : ZC_DIM]\n","    q_cat = q[:, : ZC_DIM]\n","    lcat = tf.nn.softmax_cross_entropy_with_logits(labels=z_cat, logits=q_cat)\n","    \n","    # uniform part\n","    z_uni = z[:, ZC_DIM: ZC_DIM + ZU_DIM]\n","    q_uni = q[:, ZC_DIM: ZC_DIM + ZU_DIM]\n","    \n","    luni  = 0.5 * tf.square(z_uni - q_uni);\n","    \n","    return tf.reduce_mean(lcat) + tf.reduce_mean(luni);\n","    \n","\n","def prepare_mnist_list(X):\n","    X = (X.astype(np.float32) - 127.5)/127.5\n","    X = X[:, :, :, None]\n","    return list(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4AmArCPIts5c","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Samplers\n","def random_uc():\n","    idxs = np.random.randint(ZC_DIM, size=BATCH_SIZE)\n","    onehot = np.zeros((BATCH_SIZE, ZC_DIM))\n","    onehot[np.arange(BATCH_SIZE), idxs] = 1\n","    return onehot      \n","\n","\n","def random_z():\n","    rez = np.zeros([BATCH_SIZE, Z_DIM])\n","    rez[:,        : ZC_DIM] = random_uc()\n","    rez[:, ZC_DIM : ]       = np.random.uniform(-1, 1, size=(BATCH_SIZE, Z_DIM - ZC_DIM))        \n","    return rez;\n","\n","\n","def static_uc(n_row):\n","    idxs = np.array(list(range(ZC_DIM)))\n","    onehot = np.zeros((n_row, ZC_DIM))\n","    onehot[np.arange(n_row), idxs] = 1\n","    return onehot \n","\n","\n","def static_z(c_idx, n_row): # c_idx = 1 -> c1, 1 to n.\n","  onehot = static_uc(n_row)\n","  cts = np.linspace(-1, 1, n_row)\n","  rez = []\n","\n","  for cls in onehot:\n","    for val in cts:\n","      current = np.zeros(Z_DIM)\n","      current[ : ZC_DIM] = cls\n","      current[ZC_DIM + (c_idx - 1) : ZC_DIM + c_idx] = val\n","      rez.append(current)\n","\n","  rez = np.array(rez).reshape((n_row * ZC_DIM, Z_DIM))\n","  return rez\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VbqSu5LuW6y","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Training loop\n","def train():\n","    # Prepare Training Data\n","    (X_train, _), (X_test, _) = mnist.load_data()\n","    X_train_list = prepare_mnist_list(X_train)\n","    X_test_list = prepare_mnist_list(X_test)\n","\n","    # Initialize Models\n","    real_data = tf.placeholder(tf.float32, (None, *IMG_DIM))\n","    z_ph = tf.placeholder(tf.float32, (None, Z_DIM))\n","    fake_data = generator_tf(z_ph, reuse=False)\n","    d_on_real_data = d_tf(real_data, reuse=False)\n","    d_on_fake_data = d_tf(fake_data)\n","    q_on_fake_data = q_tf(fake_data, reuse=False)\n","\n","    alpha = tf.random_uniform(shape=[tf.shape(fake_data)[0], 1, 1, 1], minval=0., maxval=1.)\n","    interpolates = real_data + alpha * (fake_data - real_data)\n","\n","    gradients = tf.gradients(d_tf(interpolates), [interpolates])[0]\n","    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1, 2, 3]))\n","    gradient_penalty = tf.reduce_mean((slopes - 1) ** 2)\n","\n","    q_cost = q_cost_tf(z_ph, q_on_fake_data)\n","    g_cost = -tf.reduce_mean(d_on_fake_data)\n","    d_cost = tf.reduce_mean(d_on_fake_data) - tf.reduce_mean(d_on_real_data) + LAMBDA * gradient_penalty\n","\n","    g_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n","    d_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n","    q_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Q')\n","\n","    g_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(g_cost, var_list=g_param)\n","    d_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(d_cost, var_list=d_param)\n","    q_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(q_cost, var_list=q_param + g_param)\n","\n","    saver = tf.train.Saver(max_to_keep=5)\n","\n","    sess = tf.Session()\n","    sess.run(tf.global_variables_initializer())\n","\n","    fix_z_c1 = static_z(1, 10)\n","    fix_z_c2 = static_z(2, 10)\n","\n","    f_train_stat = open(\"train_log.txt\", \"w\", buffering=1)\n","    f_test_stat = open(\"test_log.txt\", \"w\", buffering=1)\n","    os.system(\"mkdir -p figs_c1\")\n","    os.system(\"mkdir -p figs_c2\")\n","\n","    for it in range(ITERS):\n","        start_time = time.time()\n","\n","        for i in range(CRITIC_ITER):\n","            data = np.array(random.sample(X_train_list, BATCH_SIZE))\n","            d_cost_rez, _ = sess.run([d_cost, d_train_op], feed_dict={real_data: data, z_ph: random_z()})\n","\n","        g_cost_rez, q_cost_rez, _, _ = sess.run([g_cost, q_cost, g_train_op, q_train_op], feed_dict={z_ph: random_z()})\n","        f_train_stat.write(\"%i %g %g %g\\n\" % (it, g_cost_rez, d_cost_rez, q_cost_rez))\n","\n","        if it % 100 == 0:\n","            # sample and save\n","            print(\"Training it: %i, time/it: %g, d_cost: %g\" % (it, (time.time() - start_time), d_cost_rez))\n","            samples = sess.run([fake_data], feed_dict={z_ph: fix_z_c1})\n","            save_images(np.squeeze(samples), 'figs_c1/samples_%.6i.png' % (it))\n","            samples = sess.run([fake_data], feed_dict={z_ph: fix_z_c2})\n","            save_images(np.squeeze(samples), 'figs_c2/samples_%.6i.png' % (it))\n","            # test\n","            data = np.array(random.sample(X_test_list, BATCH_SIZE))\n","            g_cost_rez, d_cost_rez, q_cost_rez = sess.run([g_cost, d_cost, q_cost],\n","                                                          feed_dict={real_data: data, z_ph: random_z()})\n","            f_test_stat.write(\"%i %g %g %g\\n\" % (it, g_cost_rez, d_cost_rez, q_cost_rez))\n","        if (it + 1) % 2000 == 0:\n","            saver.save(sess, 'save/model', global_step=it)\n","\n","    saver.save(sess, 'save/final-model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxZh_kFR0Tu2","colab_type":"code","colab":{}},"source":["train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOWL0njd3uOi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":783},"outputId":"d8caae50-5db2-43c5-90bc-b20720dff560","executionInfo":{"status":"ok","timestamp":1580595562713,"user_tz":360,"elapsed":20209,"user":{"displayName":"Ege BeyazÄ±t","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDqLV_4GuV8zQApBx0FQZnBwgtdNoIo9u8C9JCluw=s64","userId":"02753041663080474765"}}},"source":["(X_train, _), (X_test, _) = mnist.load_data()\n","X_train_list = prepare_mnist_list(X_train)\n","X_test_list = prepare_mnist_list(X_test)\n","\n","# Initialize Models\n","real_data = tf.placeholder(tf.float32, (None, *IMG_DIM))\n","z_ph = tf.placeholder(tf.float32, (None, Z_DIM))\n","fake_data = generator_tf(z_ph, reuse=False)\n","d_on_real_data = d_tf(real_data, reuse=False)\n","d_on_fake_data = d_tf(fake_data)\n","q_on_fake_data = q_tf(fake_data, reuse=False)\n","\n","alpha = tf.random_uniform(shape=[tf.shape(fake_data)[0], 1, 1, 1], minval=0., maxval=1.)\n","interpolates = real_data + alpha * (fake_data - real_data)\n","\n","gradients = tf.gradients(d_tf(interpolates), [interpolates])[0]\n","slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1, 2, 3]))\n","gradient_penalty = tf.reduce_mean((slopes - 1) ** 2)\n","\n","q_cost = q_cost_tf(z_ph, q_on_fake_data)\n","g_cost = -tf.reduce_mean(d_on_fake_data)\n","d_cost = tf.reduce_mean(d_on_fake_data) - tf.reduce_mean(d_on_real_data) + LAMBDA * gradient_penalty\n","\n","g_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n","d_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n","q_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Q')\n","\n","g_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(g_cost, var_list=g_param)\n","d_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(d_cost, var_list=d_param)\n","q_train_op = tf.train.RMSPropOptimizer(learning_rate=1e-3).minimize(q_cost, var_list=q_param + g_param)\n","\n","saver = tf.train.Saver(max_to_keep=5)\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","fix_z_c1 = static_z(1, 10)\n","fix_z_c2 = static_z(2, 10)\n","\n","f_train_stat = open(\"train_log.txt\", \"w\", buffering=1)\n","f_test_stat = open(\"test_log.txt\", \"w\", buffering=1)\n","os.system(\"mkdir -p figs_c1\")\n","os.system(\"mkdir -p figs_c2\")\n","\n","for it in range(ITERS):\n","    start_time = time.time()\n","\n","    for i in range(CRITIC_ITER):\n","        data = np.array(random.sample(X_train_list, BATCH_SIZE))\n","        d_cost_rez, _ = sess.run([d_cost, d_train_op], feed_dict={real_data: data, z_ph: random_z()})\n","\n","    g_cost_rez, q_cost_rez, _, _ = sess.run([g_cost, q_cost, g_train_op, q_train_op], feed_dict={z_ph: random_z()})\n","    f_train_stat.write(\"%i %g %g %g\\n\" % (it, g_cost_rez, d_cost_rez, q_cost_rez))\n","\n","    if it % 100 == 0:\n","        # sample and save\n","        print(\"Training it: %i, time/it: %g, d_cost: %g\" % (it, (time.time() - start_time), d_cost_rez))\n","        samples = sess.run([fake_data], feed_dict={z_ph: fix_z_c1})\n","        save_images(np.squeeze(samples), 'figs_c1/samples_%.6i.png' % (it))\n","        samples = sess.run([fake_data], feed_dict={z_ph: fix_z_c2})\n","        save_images(np.squeeze(samples), 'figs_c2/samples_%.6i.png' % (it))\n","        # test\n","        data = np.array(random.sample(X_test_list, BATCH_SIZE))\n","        g_cost_rez, d_cost_rez, q_cost_rez = sess.run([g_cost, d_cost, q_cost],\n","                                                      feed_dict={real_data: data, z_ph: random_z()})\n","        f_test_stat.write(\"%i %g %g %g\\n\" % (it, g_cost_rez, d_cost_rez, q_cost_rez))\n","    if (it + 1) % 2000 == 0:\n","        saver.save(sess, 'save/model', global_step=it)\n","\n","saver.save(sess, 'save/final-model')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","WARNING:tensorflow:From <ipython-input-3-c583e0d2ec34>:39: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From <ipython-input-3-c583e0d2ec34>:42: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv2DTranspose` instead.\n","WARNING:tensorflow:From <ipython-input-3-c583e0d2ec34>:52: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.keras.layers.Conv2D` instead.\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","WARNING:tensorflow:From <ipython-input-3-c583e0d2ec34>:77: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Training it: 0, time/it: 7.32813, d_cost: 8.4656\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'save/final-model'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"HnZU316w1fmR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4ae2927b-fffb-4b37-8f6e-e5f6b45f6227","executionInfo":{"status":"ok","timestamp":1580595569084,"user_tz":360,"elapsed":228,"user":{"displayName":"Ege BeyazÄ±t","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDqLV_4GuV8zQApBx0FQZnBwgtdNoIo9u8C9JCluw=s64","userId":"02753041663080474765"}}},"source":["q_tf"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.q_tf>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"pqammlP01mCL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}